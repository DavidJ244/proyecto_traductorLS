# INSTRUCCIONES PARA INTEGRAR MODELOS REALES

**Para:** Desarrollador de IA  
**De:** Desarrollador Backend  
**Fecha:** 2025-11-17  
**Estado:** Backend LISTO y PROBADO ‚úÖ

---

## Resumen

El backend est√° **100% funcional** con modelos MOCK (simulados). Tu tarea es:

1. Reemplazar los servicios MOCK con tus modelos reales de IA
2. Probar el pipeline completo
3. Desplegar en AWS

---

## Estructura del Proyecto

```
backend/
‚îú‚îÄ‚îÄ app.py                    ‚Üê Servidor FastAPI (NO TOCAR)
‚îú‚îÄ‚îÄ config.py                 ‚Üê Configuraci√≥n (NO TOCAR)
‚îú‚îÄ‚îÄ .env.local                ‚Üê Variables de entorno (AJUSTAR)
‚îú‚îÄ‚îÄ requirements.txt          ‚Üê Dependencias Python
‚îú‚îÄ‚îÄ models_schemas.py         ‚Üê Modelos Pydantic (NO TOCAR)
‚îú‚îÄ‚îÄ ai_services.py            ‚Üê ‚òÖ‚òÖ‚òÖ TUS MODELOS VAN AQU√ç ‚òÖ‚òÖ‚òÖ
‚îú‚îÄ‚îÄ video_processor.py        ‚Üê Orquestador (NO TOCAR)
‚îú‚îÄ‚îÄ video_routes.py           ‚Üê Endpoints (NO TOCAR)
‚îú‚îÄ‚îÄ health_routes.py          ‚Üê Health checks (NO TOCAR)
‚îú‚îÄ‚îÄ API_DOCUMENTATION.md      ‚Üê Documentaci√≥n de endpoints
‚îî‚îÄ‚îÄ uploads/
    ‚îú‚îÄ‚îÄ videos/
    ‚îî‚îÄ‚îÄ results/
```

---

## ¬øD√≥nde van tus modelos?

### Archivo: `backend/ai_services.py`

Este archivo tiene **3 servicios** que est√°n en modo MOCK:

#### 1. Path Detection Service (~l√≠nea 50-120)

**Tu modelo:** Detecta pose/movimientos del cuerpo en el video

**Input esperado:**
```python
video_path: str  # Ruta al archivo .mp4
```

**Output esperado:**
```python
{
    "success": True,
    "keypoints": [[x1, y1, z1], [x2, y2, z2], ...],  # Lista de keypoints por frame
    "confidence": 0.95,  # 0.0 - 1.0
    "frames_processed": 300,
    "detection_time_ms": 2500
}
```

**C√≥digo actual (MOCK):**
```python
class PathDetectionServiceImpl(PathDetectionService):
    async def detect_pose_from_video(self, video_path: str) -> Dict:
        # ‚Üê REEMPLAZA ESTO CON TU MODELO
        await asyncio.sleep(2.5)  # Simula procesamiento
        return {...}  # Datos simulados
```

**C√≥mo integrarlo:**
1. Carga tu modelo entrenado (MediaPipe, OpenPose, etc.)
2. Procesa el video frame por frame
3. Extrae keypoints
4. Retorna el diccionario con el formato esperado

---

#### 2. Gloss Generator Service (~l√≠nea 125-200)

**Tu modelo:** Genera glosa (texto de se√±as) a partir de keypoints

**Input esperado:**
```python
keypoints: List[List[float]]  # Keypoints del Path Detection
```

**Output esperado:**
```python
{
    "success": True,
    "gloss": "CASA TECHO GATO ESTAR-AH√ç",  # Glosa generada
    "confidence": 0.92,  # 0.0 - 1.0
    "processing_time_ms": 1800
}
```

**C√≥digo actual (MOCK):**
```python
class GlossGeneratorServiceImpl(GlossGeneratorService):
    async def generate_gloss(self, keypoints: List[List[float]]) -> Dict:
        # ‚Üê REEMPLAZA ESTO CON TU MODELO RECURRENTE (LSTM/Transformer)
        await asyncio.sleep(1.8)
        return {...}
```

**C√≥mo integrarlo:**
1. Carga tu modelo recurrente (LSTM, Transformer, etc.)
2. Procesa la secuencia de keypoints
3. Genera la glosa
4. Retorna el diccionario con el formato esperado

---

#### 3. Text Translation Service (~l√≠nea 205-280)

**Tu modelo:** Traduce glosa a espa√±ol natural

**Input esperado:**
```python
gloss: str  # Por ejemplo: "CASA TECHO GATO ESTAR-AH√ç"
```

**Output esperado:**
```python
{
    "success": True,
    "translation": "El gato est√° en el techo de la casa",
    "confidence": 0.88,  # 0.0 - 1.0
    "processing_time_ms": 950
}
```

**C√≥digo actual (MOCK):**
```python
class TextTranslationServiceImpl(TextTranslationService):
    async def translate_gloss_to_text(self, gloss: str) -> Dict:
        # ‚Üê REEMPLAZA ESTO CON TU MODELO GENERADOR (T5/MarianMT/LLM)
        await asyncio.sleep(0.95)
        return {...}
```

**C√≥mo integrarlo:**
1. Carga tu modelo generador de texto (T5, MarianMT, etc.)
2. Traduce la glosa a espa√±ol
3. Retorna el diccionario con el formato esperado

---

## Flujo Completo (c√≥mo funciona)

```
VIDEO SUBIDO
    ‚Üì
1. Path Detection (tu modelo 1)
    ‚îî‚îÄ Detecta keypoints
    ‚Üì
2. Gloss Generator (tu modelo 2)
    ‚îî‚îÄ Genera glosa
    ‚Üì
3. Text Translation (tu modelo 3)
    ‚îî‚îÄ Traduce a espa√±ol
    ‚Üì
RESULTADO FINAL
```

---

## Setup Local

### Requisitos

- Python 3.11 o 3.12 (recomendado para compatibilidad con MediaPipe si lo usas)
- Git

### Instalaci√≥n

```bash
# 1. Clonar repositorio
git clone https://github.com/TU_USUARIO/proyecto-sena-traduccion.git
cd proyecto-sena-traduccion/backend

# 2. Crear entorno virtual
python -m venv venv

# Windows:
.\venv\Scripts\Activate.ps1
# macOS/Linux:
source venv/bin/activate

# 3. Instalar dependencias
pip install -r requirements.txt

# 4. Configurar variables de entorno
cp .env.example .env.local
# Edita .env.local con tu configuraci√≥n

# 5. Ejecutar servidor
python app.py
```

Deber√≠as ver:
```
INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)
```

---

## Testing

### Con Swagger UI (recomendado)

1. Abre `http://localhost:8000/docs`
2. Prueba el flujo completo:
   - POST /api/upload-video
   - POST /api/process-video/{job_id}
   - GET /api/status/{job_id}
   - GET /api/result/{job_id}

### Con cURL

```bash
# 1. Subir video
curl -X POST http://localhost:8000/api/upload-video -F "file=@test.mp4"
# Copiar job_id

# 2. Procesar
curl -X POST http://localhost:8000/api/process-video/JOB_ID

# 3. Consultar estado
curl http://localhost:8000/api/status/JOB_ID

# 4. Obtener resultado
curl http://localhost:8000/api/result/JOB_ID
```

---

## Checklist de Integraci√≥n

- [ ] Clon√© el repositorio
- [ ] Instal√© las dependencias
- [ ] El backend corre sin errores (con MOCKs)
- [ ] Prob√© el flujo completo en Swagger
- [ ] Entiendo d√≥nde van mis modelos (`ai_services.py`)
- [ ] Cargu√© mi modelo de Path Detection
- [ ] Cargu√© mi modelo de Gloss Generator
- [ ] Cargu√© mi modelo de Text Translation
- [ ] Prob√© el flujo completo con modelos reales
- [ ] Todo funciona end-to-end

---

## Deployment a AWS

Una vez que tus modelos funcionen localmente:

1. Crear instancia EC2 en AWS Academy
2. Instalar Python 3.11
3. Clonar repositorio
4. Configurar `.env.production`
5. Instalar dependencias
6. Configurar Nginx como reverse proxy
7. Usar systemd para mantener el servidor corriendo
8. (Opcional) Configurar S3 para almacenamiento de videos

---

## Dudas o Problemas

Si tienes dudas:

1. Lee `API_DOCUMENTATION.md` (documentaci√≥n completa de endpoints)
2. Lee `README.md` (gu√≠a general del proyecto)
3. Revisa los logs del backend (se muestran en la terminal)
4. Contacta al desarrollador backend

---

## Estado Actual

‚úÖ Backend funcional  
‚úÖ API REST documentada  
‚úÖ Pipeline MOCK probado  
‚úÖ Swagger UI funcional  
‚úÖ Health checks funcionando  
‚è≥ Modelos reales pendientes (TU TAREA)  

---

**¬°√âxito con la integraci√≥n!** üöÄ
